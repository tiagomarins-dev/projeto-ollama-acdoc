# Ollama Multi-Model API + RAG (Setup Manual Atualizado)

Este projeto permite rodar mÃºltiplos modelos LLM localmente com **Ollama**, expor uma API completa via **FastAPI** e utilizar **RAG** para consultas inteligentes em documentos.  
Agora tambÃ©m disponÃ­vel com **upload remoto de documentos** e **instruÃ§Ãµes de agentes personalizados**.

---

## ğŸš€ Funcionalidades

- Executa modelos locais via Ollama
- API para gerar textos (`/gerar`)
- API para consultar documentos (`/rag`)
- Upload remoto de documentos (`/upload_doc`)
- Cadastro remoto de instruÃ§Ãµes de agentes (`/upload_instrucoes`)
- RAG (Retrieval-Augmented Generation) integrado
- Setup 100% automatizado
- Pronto para produÃ§Ã£o
- Scripts de watchdog (auto-restart) e start rÃ¡pido

---

## ğŸ“¦ Requisitos

- Ubuntu Server 20.04+ ou superior
- Python 3.8+
- PermissÃµes de root
- Docker (opcional para quem quiser rodar com docker-compose)
- Ollama instalado

---

## ğŸ› ï¸ InstalaÃ§Ã£o

### 1. Baixar projeto

```bash
git clone https://github.com/seurepo/projeto-ollama-acdoc.git
cd projeto-ollama-acdoc
```

### 2. Criar ambiente virtual

```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Instalar dependÃªncias

```bash
pip install -r requirements.txt
```

### 4. Instalar Ollama

```bash
curl https://ollama.com/install.sh | sh
```

### 5. Baixar modelo necessÃ¡rio

```bash
ollama pull mistral
```

### 6. Iniciar API

```bash
./start.sh
```

---

## ğŸ“¡ Endpoints disponÃ­veis

### 1. Gerar texto com modelo LLM

**POST** `/gerar`

**Query Params:**
- `modelo` (obrigatÃ³rio): nome do modelo (ex: `mistral`)
- `tokens` (opcional): limite de tokens gerados (default = 300)

**Body:**
```json
{
  "prompt": "Explique a inteligÃªncia artificial."
}
```

---

### 2. Consultar documentos (RAG)

**POST** `/rag`

**Body:**
```json
{
  "prompt": "Qual o tema principal dos documentos enviados?"
}
```

---

### 3. Upload de documento para RAG

**POST** `/upload_doc`

**Form-Data:**
- `file`: Arquivo `.txt` para adicionar ao RAG

---

### 4. Upload de instruÃ§Ãµes de agente

**POST** `/upload_instrucoes`

**Form-Data:**
- `file`: Arquivo `.txt` contendo instruÃ§Ãµes especÃ­ficas do agente

---

## ğŸ“ Estrutura do Projeto

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # API principal
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ ollama_service.py   # GeraÃ§Ã£o via Ollama
â”‚   â”‚   â””â”€â”€ rag_service.py      # Mecanismo RAG
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ prompt_request.py   # Modelo de requisiÃ§Ã£o FastAPI
â”‚   â””â”€â”€ storage/
â”‚       â”œâ”€â”€ documentos/         # Documentos enviados
â”‚       â””â”€â”€ instrucoes/          # InstruÃ§Ãµes de agentes
â”œâ”€â”€ start.sh                   # Start automÃ¡tico
â”œâ”€â”€ watchdog.sh                # Watchdog auto-restart
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
```

---

## ğŸ“š Como Monitorar com Watchdog

Para garantir que o servidor UVicorn reinicie se cair:

```bash
./watchdog.sh
```

---

## ğŸ“š Modelos compatÃ­veis

- `mistral`
- `llama2`
- `llama3`
- `deepseek-coder`
- `gemma`
- `phi`
- `phi4`
- `mixtral`
- `command-r`
- `wizardlm2`
- `tinyllama`
- Outros modelos compatÃ­veis com Ollama...

---

## âœ¨ Autor

Criado com â¤ï¸ por **Tiago Marins**.

---


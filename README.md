# Ollama Multi-Model API

Este projeto permite rodar mÃºltiplos modelos LLM localmente com **Ollama** e expor uma API simples via **FastAPI**.  
VocÃª pode consultar modelos como `mistral`, `llama2` e `deepseek-chat` usando um Ãºnico endpoint HTTP.

---

## ğŸš€ Funcionalidades

- Executa modelos locais com Ollama
- API unificada para mÃºltiplos modelos
- Totalmente dockerizado
- Ideal para uso com FastAPI + RAG

---

## ğŸ“¦ Requisitos

- Docker
- Docker Compose
- Ollama (instalado via container)

---

## ğŸ§ª Como usar

### 1. Suba os containers:

```bash
docker-compose up --build -d
```

Isso iniciarÃ¡ a API em `http://localhost:8000` e o Ollama em `http://localhost:11434`.

### 2. FaÃ§a uma requisiÃ§Ã£o POST:

**Endpoint:**

```
POST /gerar?modelo=mistral
```

**Body JSON:**

```json
{
  "prompt": "Explique a teoria da relatividade."
}
```

VocÃª pode substituir `modelo` por:
- `mistral`
- `llama2`
- `deepseek-chat`

---

## ğŸ›  Arquitetura

- **FastAPI** para expor a API
- **Ollama** para gerar as respostas localmente
- **Docker Compose** para orquestrar tudo

---

## ğŸ“ Estrutura

```
â”œâ”€â”€ main.py              # API FastAPI
â”œâ”€â”€ Dockerfile           # Ambiente Python
â”œâ”€â”€ docker-compose.yml   # Orquestra API + Ollama
â”œâ”€â”€ requirements.txt     # DependÃªncias Python
â””â”€â”€ README.md            # Este arquivo
```

---

## ğŸ§  Dica

Se estiver usando cloud-init (como na Hetzner), clone este repositÃ³rio automaticamente e execute o `docker-compose up` no boot do servidor.

---

## âœ¨ Autor

Criado por Tiago com â¤ï¸ e LLMs.
# Ollama Multi-Model API (com Setup Manual)

Este projeto permite rodar mÃºltiplos modelos LLM localmente com **Ollama** e expor uma API simples via **FastAPI**.  
Agora tambÃ©m disponÃ­vel com **instalaÃ§Ã£o completa via script `.sh`**.

---

## ğŸš€ Funcionalidades

- Executa modelos locais com Ollama
- API unificada para mÃºltiplos modelos
- Totalmente dockerizado
- Setup 100% automatizado com script bash
- Ideal para uso com FastAPI + RAG

---

## ğŸ“¦ Requisitos

- Ubuntu Server 20.04+ ou superior
- PermissÃµes de root para instalar pacotes

---

## ğŸ› ï¸ InstalaÃ§Ã£o AutomÃ¡tica

1. FaÃ§a login no seu servidor via SSH.
2. Clone este repositÃ³rio ou crie o arquivo `setup-ollama-server.sh`.
3. DÃª permissÃ£o de execuÃ§Ã£o:

```bash
chmod +x setup-ollama-server.sh
```

4. Execute:

```bash
./setup-ollama-server.sh
```

5. Acesse a API:

```
http://SEU_IP:8000/docs
```

---

## ğŸ“¡ Como usar a API

### Endpoint `/gerar`

**MÃ©todo:** `POST`

**URL:** 
```
/gerar?modelo=mistral&tokens=200
```

**Body JSON:**

```json
{
  "prompt": "Explique a inteligÃªncia artificial."
}
```

**ParÃ¢metros disponÃ­veis:**
- `modelo` (query param) â€” Exemplo: `mistral`, `llama2`, `deepseek-chat`
- `tokens` (query param) â€” Limite de geraÃ§Ã£o de tokens (opcional, padrÃ£o 300)

---

## ğŸ“ Estrutura do Projeto

```
â”œâ”€â”€ main.py              # API FastAPI
â”œâ”€â”€ Dockerfile           # Ambiente Python
â”œâ”€â”€ docker-compose.yml   # Orquestra API + Ollama
â”œâ”€â”€ requirements.txt     # DependÃªncias Python
â”œâ”€â”€ setup-ollama-server.sh # Script de instalaÃ§Ã£o completa
â””â”€â”€ README.md            # Este arquivo
```

---

## âœ¨ Autor

Criado por Tiago Marins.

# Ollama Multi-Model API (com Setup Manual)

Este projeto permite rodar mÃºltiplos modelos LLM localmente com **Ollama** e expor uma API simples via **FastAPI**.  
Agora tambÃ©m disponÃ­vel com **instalaÃ§Ã£o completa via script `.sh`**.

---

## ğŸš€ Funcionalidades

- Executa modelos locais com Ollama
- API unificada para mÃºltiplos modelos
- Totalmente dockerizado
- Setup 100% automatizado com script bash
- Ideal para uso com FastAPI + RAG

---

## ğŸ“¦ Requisitos

- Ubuntu Server 20.04+ ou superior
- PermissÃµes de root para instalar pacotes

---

## ğŸ› ï¸ InstalaÃ§Ã£o AutomÃ¡tica

1. FaÃ§a login no seu servidor via SSH.
2. Clone este repositÃ³rio ou crie o arquivo `setup-ollama-server.sh`.
3. DÃª permissÃ£o de execuÃ§Ã£o:

```bash
chmod +x setup-ollama-server.sh
```

4. Execute:

```bash
./setup-ollama-server.sh
```

5. Acesse a API:

```
http://SEU_IP:8000/docs
```

---

## ğŸ“¡ Como usar a API

### Endpoint `/gerar`

**MÃ©todo:** `POST`

**URL:** 
```
/gerar?modelo=mistral&tokens=200
```

**Body JSON:**

```json
{
  "prompt": "Explique a inteligÃªncia artificial."
}
```

**ParÃ¢metros disponÃ­veis:**
- `modelo` (query param) â€” Exemplo: `mistral`, `llama2`, `deepseek-chat`
- `tokens` (query param) â€” Limite de geraÃ§Ã£o de tokens (opcional, padrÃ£o 300)

---

## ğŸ“ Estrutura do Projeto

```
â”œâ”€â”€ main.py              # API FastAPI
â”œâ”€â”€ Dockerfile           # Ambiente Python
â”œâ”€â”€ docker-compose.yml   # Orquestra API + Ollama
â”œâ”€â”€ requirements.txt     # DependÃªncias Python
â”œâ”€â”€ setup-ollama-server.sh # Script de instalaÃ§Ã£o completa
â””â”€â”€ README.md            # Este arquivo
```


---

## ğŸ“š Modelos disponÃ­veis para uso na API

Todos os modelos abaixo podem ser usados assim:

```
POST /gerar?modelo=nome-do-modelo&tokens=200
```

**Exemplo:**
```
/gerar?modelo=phi4&tokens=300
```

### âœ… Modelos instalados com o script:

- `mistral` â†’ `?modelo=mistral`
- `llama2` â†’ `?modelo=llama2`
- `llama3` â†’ `?modelo=llama3`
- `llama3:8b` â†’ `?modelo=llama3`
- `llama3:70b` â†’ `?modelo=llama3`
- `deepseek-coder` â†’ `?modelo=deepseek-coder`
- `deepseek-r1:7b` â†’ `?modelo=deepseek-r1`
- `deepseek-r1:14b` â†’ `?modelo=deepseek-r1`
- `gemma:2b` â†’ `?modelo=gemma`
- `gemma3` â†’ `?modelo=gemma3`
- `phi` â†’ `?modelo=phi`
- `phi4` â†’ `?modelo=phi4`
- `phi3` â†’ `?modelo=phi3`
- `phi3.5` â†’ `?modelo=phi3.5`
- `qwen2.5:7b` â†’ `?modelo=qwen2.5`
- `qwen2.5:14b` â†’ `?modelo=qwen2.5`
- `qwen2.5-coder:7b` â†’ `?modelo=qwen2.5-coder`
- `mixtral:8x7b` â†’ `?modelo=mixtral`
- `mixtral:8x22b` â†’ `?modelo=mixtral`
- `dolphin-llama3:8b` â†’ `?modelo=dolphin-llama3`
- `starcoder2:7b` â†’ `?modelo=starcoder2`
- `codellama:7b` â†’ `?modelo=codellama`
- `command-r` â†’ `?modelo=command-r`
- `wizardlm2:7b` â†’ `?modelo=wizardlm2`
- `tinyllama` â†’ `?modelo=tinyllama`

---

## âœ¨ Autor

Criado por Tiago Marins.
